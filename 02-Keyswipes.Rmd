# Data Processing - Keyswipes Data

1.	In -> CORTEX II/Data/PrimaryData-Attendance+Fitbit/Visit Data"
* Opened up each waves raw file
* Then opened up every individual ID file and added a column for their ID #
  + Checked to make sure all data was refinery check in. 
    1.	Anything else removed (e.g., childcare, corporate, WEST)
  + Saved as: ID101.xlsx
  + Previous raw file was kept as is and placed into Data\WaveX\Visit_Data\WaveX_visit data (mainly for Tiffs W1)

2.	**Visits_pre.R**
* Here we loaded in ALL the excel files in a specific folder and then bind them together in one data frame 
* ** Wave3, for some reason this data was received differently from the refinery and is reflected in certain individual files. Use raw file BE FIT Dec – Jan.xls for reference.
  + Files for ID’s 208, 211, 213, 222, 251 were deleted due to being blank
  + ID’s 232, 224/225, 245, 256 had data in feb/2019 removed
  + Wave4: Files for ID’s 214, 264, 282 were deleted due to being blank
  
```{r eval = FALSE,  linewidth=50}
##### Libraries #####
library(readxl)
library(plyr)    # for binding multiple files
library(dplyr)
library(stringr) # for function str_replace_all


##### Loading in multiple files and binding together ####
setwd("C:/Users/jadamek2/Desktop/Visits/wave1")
wave1 <- list.files(pattern = "*.xlsx", full.names = T)
wave1 <- sapply(wave1, read_excel, simplify=FALSE) %>% bind_rows(.id = "ID")  # binding files together in one dataframe

setwd("C:/Users/jadamek2/Desktop/Visits/wave2")
wave2 <- list.files(pattern = "*.xlsx", full.names = T)
wave2 <- sapply(wave2, read_excel, simplify=FALSE) %>% bind_rows(.id = "Client")

setwd("C:/Users/jadamek2/Desktop/Visits/wave3")
wave3 <- list.files(pattern = "*.xlsx", full.names = T)
wave3 <- sapply(wave3, read_excel, simplify=FALSE) %>% bind_rows(.id = "Client")

setwd("C:/Users/jadamek2/Desktop/Visits/wave4")
wave4 <- list.files(pattern = "*.xlsx", full.names = T)
wave4 <- sapply(wave4, read_excel, simplify=FALSE) %>% bind_rows(.id = "Client")

setwd("C:/Users/jadamek2/Desktop/Visits/wave5")
wave5 <- list.files(pattern = "*.xlsx", full.names = T)
wave5 <- sapply(wave5, read_excel, simplify=FALSE) %>% bind_rows(.id = "Client")

setwd("C:/Users/jadamek2/Desktop/Visits/wave6")
wave6 <- list.files(pattern = "*.xlsx", full.names = T)
wave6 <- sapply(wave6, read_excel, simplify=FALSE) %>% bind_rows(.id = "Client")
```


3.	**Visits_pre.R:** ##### Data cleaning #####
* Changed class type for wave1$Date to match other waves
* Cleaned the ID variable to remove the file name (e.g., ./215.xlsx -> 215)
* Selected the needed columns (date,time,Id,payment)
* Selected only visits from Be Fit Study. This ensured that things like childcare, corporate, or WEST swipes were not included in the database
* Data was saved by wave as: wave1_cleaned.csv
* File was open
* ** Notes: 
  + ID 285  7/5/2018 less than 4 hours
  + ID 217 3/3/2018 less than 4 hours
  + ID 217 3/17/2018 less than 4 hours
  + ID 218 3/20/2018 less than 4 hours
  + ID 218 3/27/2018 less than 4 hours
  + ID 218 3/29/2018 less than 4 hours
  + ID 218 4/9/2018 less than 4 hours
  + ID 218 4/16/2018 less than 4 hours
  + ID 218 11/26/2018 less than 4 hours
  + ID 229 3/31/2018 less than 4 hours

```{r eval = FALSE, linewidth=50}
##### Libraries #####
library(readxl)
library(plyr)    # for binding multiple files
library(dplyr)
library(stringr) # for function str_replace_all


##### Data Cleaning ####

# Converts class type for Date to character for wave 1 so that its the same as wave 2-6
wave1$Date <- as.character(wave1$Date)

# Changes column name from Client to ID
colnames(wave2)[5] <- "ID"
colnames(wave3)[5] <- "ID"
colnames(wave4)[5] <- "ID"
colnames(wave5)[5] <- "ID"
colnames(wave6)[5] <- "ID"

# Clean ID column to only show ID# and not file extension 
wave1$ID <- gsub(".*D","", wave1$ID)                # Removes everything before the number | gsub =before
wave1$ID <- wave1$ID %>% str_replace_all('\\.', '') # Removes the period in the column
wave1$ID <- sub("xlsx","", wave1$ID)                # Removes everything after the number   | sub = after

wave2$ID <- gsub("/", "", wave2$ID)                 # Removes the / before the data
wave2$ID <- wave2$ID %>% str_replace_all('\\.', '') 
wave2$ID <- sub("xlsx","", wave2$ID)

wave3$ID <- gsub("/","", wave3$ID)                
wave3$ID <- wave3$ID %>% str_replace_all('\\.', '') 
wave3$ID <- sub("xlsx","", wave3$ID) 

wave4$ID <- gsub("/", "", wave4$ID)                
wave4$ID <- wave4$ID %>% str_replace_all('\\.', '') 
wave4$ID <- sub("xlsx","", wave4$ID) 

wave5$ID <- gsub("/", "", wave5$ID)                
wave5$ID <- wave5$ID %>% str_replace_all('\\.', '') 
wave5$ID <- sub("xlsx","", wave5$ID) 

wave6$ID <- gsub("/", "", wave6$ID)                
wave6$ID <- wave6$ID %>% str_replace_all('\\.', '') 
wave6$ID <- sub("xlsx","", wave6$ID) 

# Removes the extra noise in the Time column
## (.*) looks for any character 0 or more times until the first space, (?) is what makes it stop at the first space
wave1$Time <- gsub(".*? ", " ", wave1$Time) 

# Changes column "Check-In Time" to "Time" to remain consistent with the other waves
colnames(wave1)[8] <- "Pricing Option"
colnames(wave2)[6] <- "Time"
colnames(wave3)[6] <- "Time"
colnames(wave4)[6] <- "Time"
colnames(wave5)[6] <- "Time"
colnames(wave6)[6] <- "Time"

# Select the needed columns (date,time,ID,Payment) then ensures dataset contains only those in Befit study
w1 <- wave1[,c(1,3,2,8)]
w1 <- subset(w1, `Pricing Option` == "Be Fit Study")

w2 <- wave2[,c(1,5,6,7)]
w2 <- subset(w2, `Pricing Option` == "Be Fit Study")

w3 <- wave3[,c(1,5,6,7)]
w3 <- subset(w3, `Pricing Option` == "Be Fit Study")

w4 <- wave4[,c(1,5,6,7)]
w4 <- subset(w4, `Pricing Option` == "Be Fit Study")

w5 <- wave5[,c(1,5,6,7)]
w5 <- subset(w5, `Pricing Option` == "Be Fit Study")

w6 <- wave6[,c(1,5,6,7)]
w6 <- subset(w6, `Pricing Option` == "Be Fit Study")

# Writes the cleaned file to Visits folder
setwd("C:/Users/jadamek2/Desktop/Visits")
write.csv(w1, "wave1_cleaned.csv")
write.csv(w2, "wave2_cleaned.csv")
write.csv(w3, "wave3_cleaned.csv")
write.csv(w4, "wave4_cleaned.csv")
write.csv(w5, "wave5_cleaned.csv")
write.csv(w6, "wave6_cleaned.csv")

```

4.	**Visits_Step1:**
* Checked for ID’s that had duplicates and saved those duplicates by wave (e.g., wave1_duplicates.csv)
* This file was open externally and dates were highlighted in the respectful wave1_cleaned.csv file
* Dates were removed if <4 hours apart, otherwise they remained the same. Final file was saved as = wave1_final.csv
  + Wave 1
    1.	ID 115, 5/27/2017 - removed
    2.	ID 115, 6/10/2017 - removed
    3.	ID 115, 6/17/2017 - removed
    4.	ID 115, 7/1/2017 - removed
    5.	ID 115, 7/15/2017 - removed
    6.	ID 115, 7/22/2017 - removed
    7.	ID 115, 7/29/2017 - removed
    8.	ID 115, 8/5/2017 - removed
    9.	ID 115, 8/12/2017 - removed
    10.	ID 116, 3/27/2017 – kept: both >4hrs apart
    11.	ID 116, 3/29/2017 – kept: both >4hrs apart
    12.	ID 116, 4/3/2017 – kept: both >4hrs apart
    13.	ID 116, 4/12/2017 – kept: both >4hrs apart
    14.	ID 116, 4/24/2017 – kept: both >4hrs apart
    15.	ID 116, 4/26/2017 – kept: both >4hrs apart
    16.	ID 132, 2/4/2017 - removed
    17.	ID 135, 7/7/2017 – kept: both >4hrs apart
    18.	ID 135, 12/20/2017 – kept: both >4hrs apart
    19.	ID 141, 2/6/2017 – kept: both >4hrs apart
    20.	ID 143, 3/3/2017 - removed
    21.	ID 143, 3/10/2017 - removed
    22.	ID 143, 4/14/2017 - removed
    23.	ID 143, 4/21/2017 - removed
    24.	ID 143, 5/5/2017 - removed
    25.	ID 143, 5/12/2017 - removed
    26.	ID 143, 5/19/2017 - removed
    27.	ID 143, 6/2/2017 - removed
    28.	ID 143, 6/16/2017 - removed
    29.	ID 143, 6/30/2017 - removed
    30.	ID 143, 7/7/2017 - removed
    31.	ID 143, 8/4/2017 - removed
    32.	ID 143, 8/11/2017 - removed
    33.	ID 143, 8/18/2017 - removed
    34.	ID 143, 9/1/2017 - removed
    35.	ID 143, 10/30/2017 - removed
    36.	ID 143, 11/6/2017 - removed
    37.	ID 143, 11/13/2017 - removed
    38.	ID 143, 11/20/2017 - removed
    39.	ID 143, 11/27/2017 - removed
    40.	ID 143, 12/4/2017 - removed
    41.	ID 143, 12/11/2017 - removed
    42.	ID 143, 12/18/2017 - removed
    43.	ID 143, 1/8/2018 - removed
    44.	ID 158, 4/24/2017 – kept: both >4hrs apart
  + Wave 2
    1.	ID 156, 3/19/2018 – kept: both >4hrs apart
    2.	ID 156, 7/24/2017 - removed
    3.	ID 156, 8/15/2017 - removed
    4.	ID 167, 8/8/2017 - removed
    5.	ID 169, 3/14/2018 – kept: both >4hrs apart
    6.	ID 176, 6/17/2017 - removed
    7.	ID 177, 9/11/2017 – kept: both >4hrs apart
    8.	ID 177, 9/2/2017 - removed
    9.	ID 178, 8/3/2017 - removed
    10.	ID 178, 8/31/2017 - removed
    11.	ID 179, 6/26/2017 - removed
    12.	ID 182, 6/29/2017 - removed
  + Wave 3
    1.	ID 215, 5/14/2018 – kept: both >4hrs apart
    2.	ID 215, 5/24/2018 – kept: both >4hrs apart
    3.	ID 215, 6/18/2018 – kept: both >4hrs apart
    4.	ID 215, 6/20/2018 – kept: both >4hrs apart
    5.	ID 215, 6/21/2018 – kept: both >4hrs apart
    6.	ID 215, 7/5/2018 – kept: both >4hrs apart
    7.	ID 217, 3/17/2018 - removed
    8.	ID 217, 3/29/2018 – kept: both >4hrs apart
    9.	ID 217, 3/3/2018 - removed
    10.	ID 217, 4/2/2018 - removed
    11.	ID 217, 6/15/2018  – kept: both >4hrs apart
    12.	ID 218, 11/14/2018 – kept: both >4hrs apart
    13.	ID 218, 11/26/2018 - removed
    14.	ID 218, 3/15/2018 – kept: both >4hrs apart
    15.	ID 218, 3/20/2018 - removed
    16.	ID 218, 3/27/2018 - removed
    17.	ID 218, 3/29/2018 – removed 
    18.	ID 218, 4/16/2018 - removed
    19.	ID 218, 4/17/2018 – kept: both >4hrs apart
    20.	ID 218, 4/9/2018 - removed
    21.	ID 218, 5/21/2018 – removed 
    22.	ID 218, 6/12/2018 - removed
    23.	ID 218, 6/21/2018 – kept: both >4hrs apart
    24.	ID 218, 6/7/2018 - removed
    25.	ID 218, 8/6/2018 – kept: both >4hrs apart
    26.	ID 224, 3/30/2018 - removed
    27.	ID 229, 3/31/2018 - removed
    28.	ID 230, 2/12/2018 - removed
    29.	ID 230, 2/14/2018 – kept: both >4hrs apart
    30.	ID 230, 2/8/2018 – kept: both >4hrs apart
    31.	ID 232, 11/23/2018 – kept: both >4hrs apart
    32.	ID 232, 3/28/2018 – kept: both >4hrs apart
    33.	ID 232, 4/11/2018 – kept: both >4hrs apart
    34.	ID 233, 3/9/2018 – kept: both >4hrs apart
    35.	ID 248, 1/2/2019 – kept: both >4hrs apart
    36.	ID 254, 3/12/2018 – kept: both >4hrs apart
    37.	ID 254, 3/26/2018 – kept: both >4hrs apart
    38.	ID 254, 4/3/2018 – removed 
    39.	ID 256, 2/6/2018 - removed
  + Wave 4/
    1.	ID 285, 7/5/2018 - removed
  + Wave 5
    1.	ID 306, 2/5/2019 – removed
    2.	ID 306, 3/19/2019 - removed
  + Wave 6
    1.	ID 358, 6/20/2019 – removed
* Each “wave1_final.csv” was checked once again for formatting (date 48256) and 12-month range.
* Wave1_final.csv is new master type file moving forward

```{r eval = FALSE,  linewidth=50}

library(dplyr)
library(tidyr)

# Set working directory and read in data
setwd("C:/Users/jadamek2/Desktop/Visits")
wave1 <- read.csv("wave1_cleaned.csv")
wave2 <- read.csv("wave2_cleaned.csv")
wave3 <- read.csv("wave3_cleaned.csv")
wave4 <- read.csv("wave4_cleaned.csv")
wave5 <- read.csv("wave5_cleaned.csv")
wave6 <- read.csv("wave6_cleaned.csv")

# Determining which ID's have duplicate check-In Dates
check <- wave1 %>% group_by(ID,Date) %>% count(Date)
w1d <- with(check, check[n > 1, ])

check <- wave2 %>% group_by(ID,Date) %>% count(Date)
w2d <- with(check, check[n > 1, ])

check <- wave3 %>% group_by(ID,Date) %>% count(Date)
w3d <- with(check, check[n > 1, ])

check <- wave4 %>% group_by(ID,Date) %>% count(Date)
w4d <- with(check, check[n > 1, ])

check <- wave5 %>% group_by(ID,Date) %>% count(Date)
w5d <- with(check, check[n > 1, ])

check <- wave6 %>% group_by(ID,Date) %>% count(Date)
w6d <- with(check, check[n > 1, ])

# Writing file of all duplicate ID's to be checked
write.csv(w1d, "wave1_duplicates.csv")
write.csv(w2d, "wave2_duplicates.csv")
write.csv(w3d, "wave3_duplicates.csv")
write.csv(w4d, "wave4_duplicates.csv")
write.csv(w5d, "wave5_duplicates.csv")
write.csv(w6d, "wave6_duplicates.csv")

##### See Data Processing Visits.docx 4.c ####
# duplicate swipes on same day was checked for 4 hour rule

# read in files after removing duplicates ####
setwd("C:/Users/jadamek2/Desktop/Visits")
wave1 <- read.csv("wave1_final.csv")
wave2 <- read.csv("wave2_final.csv")
wave3 <- read.csv("wave3_final.csv")
wave4 <- read.csv("wave4_final.csv")
wave5 <- read.csv("wave5_final.csv")
wave6 <- read.csv("wave6_final.csv")
```


**** NEXT STEP IS TO MERGE QR AND VISITS TOGETHER BASED ON ID ****
    